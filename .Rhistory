#Converting the date data to text
date_data <- html_text(date_data_html)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |     "
strsplit(split =pattern,x = date_data)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |     |Posté dans  Actualité   le  "
strsplit(split =pattern,x = date_data)
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |     |Posté dans  Actualité   le  |   "
strsplit(split =pattern,x = date_data)
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
strsplit(split =pattern,x = date_data)
df$X5[[2]]
url <- "https://www.lkeria.com/actualité/immobilier/le-marche-immobilier-en-espagne-continue-a-seduire-les-etrangers"
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
date_data_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
date_data <- html_text(date_data_html)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
strsplit(split =pattern,x = date_data)
df$X5[[10]]
url <- "https://www.lkeria.com/actualité/aadl/l-apoce-depose-pleinte-contre-l-aadl-et-l-enpi"
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
date_data_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
date_data <- html_text(date_data_html)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
strsplit(split =pattern,x = date_data)
df$X5[[26]]
url <- "https://www.lkeria.com/actualité/logement/relogment-troiseieme-phases"
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
date_data_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
date_data <- html_text(date_data_html)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
df$X5[[96]]
url <- "https://www.lkeria.com/actualité/people/justin-biber-payerai-un-loyer-d-un-milliards-par-mois-pour-habiter-bverly-hill"
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
date_data_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
date_data <- html_text(date_data_html)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
strsplit(split =pattern,x = date_data)
pattern <- pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
pattern_r <- "\\(adsbygoogle=window.adsbygoogle\\|\\|\\[\\]\\).push\\(\\{\\}\\);|.pagespeed.lazyLoadImages.overrideAttributeFunctions\\(\\);"
lkeria_scrapper <- function(url){
print(url)
if(url.exists(url = url)== F){
return(c(NULL,NULL,NULL,NULL,NULL))
}else{
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
raw_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
raw_data <- html_text(raw_html)
rawlist <- unlist((strsplit(x = raw_data,split = pattern)),use.names = F)
if(length(rawlist)==6){
title <- trim(rawlist[1])
date <- rawlist[2]
writer <- rawlist[3]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[4])
}else{
title <- trim(rawlist[1])
date <- rawlist[3]
writer <- rawlist[4]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[5])
}
return(c(title,date,writer,text,url))
}}
The_Data <- lapply(TheLinks, lkeria_scrapper)
df <- data.frame(matrix(unlist(The_Data), nrow=1058, byrow=T),stringsAsFactors=FALSE)
View(df)
df$X5[[148]]
df$X5[[230]]
df$X5[[966]]
df$X4[[966]]
df$X4[[997]]
df$X4[[1040]]
df$X5[[1040]]
url <- "https://www.lkeria.com/actualit%C3%A9/logement/un-parlementaire-viole-la-loi"
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
date_data_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
date_data <- html_text(date_data_html)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
strsplit(split =pattern,x = date_data)
df$X5[[477]]
url <- "https://www.lkeria.com/actualité/logement/AADL2-interview-de-Lkeria-DG-AADL"
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
date_data_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
date_data <- html_text(date_data_html)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
strsplit(split =pattern,x = date_data)
pattern <- pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
pattern_r <- "\\(adsbygoogle=window.adsbygoogle\\|\\|\\[\\]\\).push\\(\\{\\}\\);|.pagespeed.lazyLoadImages.overrideAttributeFunctions\\(\\);"
lkeria_scrapper <- function(url){
print(url)
if(url.exists(url = url)== F){
return(c(NULL,NULL,NULL,NULL,NULL))
}else{
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
raw_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
raw_data <- html_text(raw_html)
rawlist <- unlist((strsplit(x = raw_data,split = pattern)),use.names = F)
if(length(rawlist)==6){
title <- trim(rawlist[1])
date <- rawlist[2]
writer <- rawlist[3]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[4])
}else if(length(rawlist)==7){
title <- trim(rawlist[1])
date <- rawlist[3]
writer <- rawlist[4]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[5])
}else{
title <- trim(rawlist[1])
date <- rawlist[3]
writer <- rawlist[4]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[6])
}
return(c(title,date,writer,text,url))
}}
The_Data <- lapply(TheLinks, lkeria_scrapper)
df <- data.frame(matrix(unlist(The_Data), nrow=1058, byrow=T),stringsAsFactors=FALSE)
View(df)
df$X5[[1042]]
url <- "https://www.lkeria.com/actualit%C3%A9/logement/les-indus-beneficiaires-de-logements-sociaux-seront-ils-poursuivis-en-justice"
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
date_data_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
date_data <- html_text(date_data_html)
date_data
pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
strsplit(split =pattern,x = date_data)
date_data
lkeria_scrapper <- function(url){
print(url)
if(url.exists(url = url)== F){
return(c(NULL,NULL,NULL,NULL,NULL))
}else{
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
raw_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
raw_data <- html_text(raw_html)
rawlist <- unlist((strsplit(x = raw_data,split = pattern)),use.names = F)
if(length(rawlist)==6){
title <- trim(rawlist[1])
date <- rawlist[2]
writer <- rawlist[3]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[4])
}else if(length(rawlist)%in% c(7,8)){
title <- trim(rawlist[1])
date <- rawlist[3]
writer <- rawlist[4]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[5])
}else{
title <- trim(rawlist[1])
date <- rawlist[3]
writer <- rawlist[4]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[6])
}
return(c(title,date,writer,text,url))
}}
The_Data <- lapply(TheLinks, lkeria_scrapper)
df <- data.frame(matrix(unlist(The_Data), nrow=1058, byrow=T),stringsAsFactors=FALSE)
The_Data <- lapply(TheLinks, lkeria_scrapper)
pattern <- pattern <- "Posté dans  Actualité   le  |  par |Article modifié le :  |   +|Posté dans  Actualité   le  "
pattern_r <- "\\(adsbygoogle=window.adsbygoogle\\|\\|\\[\\]\\).push\\(\\{\\}\\);|.pagespeed.lazyLoadImages.overrideAttributeFunctions\\(\\);"
lkeria_scrapper <- function(url){
print(url)
if(url.exists(url = url)== F){
return(c(NULL,NULL,NULL,NULL,NULL))
}else{
webpage <- read_html(url)
#Using CSS selectors to scrap the dates section
raw_html <- html_nodes(webpage,'.content-wrap')
#Converting the date data to text
raw_data <- html_text(raw_html)
rawlist <- unlist((strsplit(x = raw_data,split = pattern)),use.names = F)
if(length(rawlist)==6){
title <- trim(rawlist[1])
date <- rawlist[2]
writer <- rawlist[3]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[4])
}else if(length(rawlist)%in% c(7,8)){
title <- trim(rawlist[1])
date <- rawlist[3]
writer <- rawlist[4]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[5])
}else{
title <- trim(rawlist[1])
date <- rawlist[3]
writer <- rawlist[4]
text <- gsub(pattern = pattern_r,replacement = " ",x = rawlist[6])
}
return(c(title,date,writer,text,url))
}}
The_Data <- lapply(TheLinks, lkeria_scrapper)
df <- data.frame(matrix(unlist(The_Data), nrow=1058, byrow=T),stringsAsFactors=FALSE)
View(df)
names(df) <- c("Title","Publication.Date","Writer","Text","Link")
write.csv(df,"Articles.csv")
strsplit(df$Link[1],split = "/")
strsplit(df$Link[1],split = "/")[[1]][5]
strsplit(df$Link,split = "/")
sapply(df$Link,FUN = function(x) strsplit(x,split = "/")[[1]][5],USE.NAMES = F)
df$Type <- sapply(df$Link,FUN = function(x) strsplit(x,split = "/")[[1]][5],USE.NAMES = F)
View(df)
table(df$Type)
df[df$Type=="jsi"]
df[df$Type=="jsi",]
get_rid <- function(x){
x.p <- strsplit(x,split = "-->")[[1]]
if(length(x.p) == 1){
return(trim(x.p[1]))
}else if(length(x.p) == 2){
return(trim(x.p[2]))
}else{return(NA)}
}
df$Text_V <- sapply(df$Text,get_rid,USE.NAMES = F)
gsub("\\[[^\\]]*\\]", "", df$Text_V[1], perl=TRUE)
gsub("\\[[^\\]]*\\]", "", df$Text_V[952], perl=TRUE)
df$Text_V[952]
get_rid <- function(x){
x.p <- strsplit(x,split = "-->")[[1]]
if(length(x.p) == 1){
return(trim(gsub("\\[[^\\]]*\\]", " ", x.p[1], perl=TRUE)[1]))
}else if(length(x.p) == 2){
return(trim(gsub("\\[[^\\]]*\\]", " ", x.p[2], perl=TRUE)[1]))
}else{return(NA)}
}
get_rid <- function(x){
x.p <- strsplit(x,split = "-->")[[1]]
if(length(x.p) == 1){
return(trim(gsub("\\[[^\\]]*\\]", " ", x.p[1], perl=TRUE)[1]))
}else if(length(x.p) == 2){
return(trim(gsub("\\[[^\\]]*\\]", " ", x.p[2], perl=TRUE)[1]))
}else{return(NA)}
}
df$Text_V <- sapply(df$Text,get_rid,USE.NAMES = F)
View(df)
View(df)
df$Link[26]
df$Publication.Date.2 <- as.Date(df$Publication.Date,format = "%d/%m/%Y")
View(df)
df <- df[!is.na(df$Publication.Date.2),]
View(df)
load("~/.RData")
setwd("~/AlgerianRUser4thMeetUp")
library(stringr)
library(rvest)
library(xml)
library(xml2)
library(rvest)
link <- "https://www.lkeria.com/actualité"
url <- "https://www.lkeria.com/actualité"
page=read_html(url)
page %>% html_nodes(".biz-name") %>% html_attr('href')
url <- "https://www.lkeria.com/actualité"
page=read_html(url)
page %>% html_nodes(".biz-name") %>% html_attr('href')
page %>% html_nodes(".standard-blog") %>% html_attr('href')
page=read_html(url)
page %>% html_nodes(".standard-blog") %>% html_attr('href')
url %>%
html_node("a")%>%
html_attr("href")
url %>%
html_nodes("a")%>%
html_attr("href")
url %>%
html_nodes("a")%>%
html_attr("href")
url <- "https://www.lkeria.com/actualité"
url %>%
html_nodes("a")%>%
html_attr("href")
url %>%
read_html()%>%
html_nodes("a")%>%
html_attr("href")
links <- sapply(paste0(url,1:3),
function(url){
url %>%
read_html()%>%
html_nodes("a")%>%
html_attr("href")
},USE.NAMES = F)
Scrap_link<- function(url){
url %>%
read_html()%>%
html_nodes("a")%>%
html_attr("href")
}
links <- sapply(paste0(url,1:3),
Scrap_link,
,USE.NAMES = F)
links <- sapply(paste0(url,1:3),
Scrap_link,
,USE.NAMES = F)
links <- sapply(paste0(url,1:3),
Scrap_link,
USE.NAMES = F)
links <- sapply(paste0(url,1:3,collapse = ""),
Scrap_link,
USE.NAMES = F)
paste0(url,1:3,collapse = "")
url <- "https://www.lkeria.com/actualité-P"
paste0(url,1:3,collapse = "")
paste(url,1:3,collapse = "")
paste0(url,1:3,collapse = "")
links <- sapply(paste0(url,1:3,collapse = ""),
Scrap_link,
USE.NAMES = F)
links <- sapply(paste0(url,c(1:3),collapse = ""),
Scrap_link,
USE.NAMES = F)
paste0(url,c(1:3))
links <- sapply(paste0(url,c(1:3)),
Scrap_link,
USE.NAMES = F)
links
lapply(links,grep,pattern="^actualité/")
sapply(links,grep,pattern="^actualité/")
links <- sapply(paste0(url,c(1:3)),
Scrap_link,
USE.NAMES = F)%>%
unlist(,use.names = F)
links <- sapply(paste0(url,c(1:3)),
Scrap_link,
USE.NAMES = F)%>%
unlist(use.names = F)
View(links)
links <- lapply(paste0(url,c(1:3)),
Scrap_link,
USE.NAMES = F)%>%
unlist(use.names = F)
links <- lapply(paste0(url,c(1:3)),
Scrap_link,
USE.NAMES = F)
unlist(links,use.names = F)
links <- lapply(paste0(url,c(1)),
Scrap_link,
USE.NAMES = F)
links <- lapply(paste0(url,c(1)),
Scrap_link,
USE.NAMES = F)
links <- lapply(paste0(url,c(1)),
Scrap_link)
View(links)
sapply(links,grep,pattern="^actualité/")
links <- lapply(paste0(url,c(1,2)),
Scrap_link)
sapply(links,grep,pattern="^actualité/")
sapply(links,grep,pattern="^actualité/")
unlist(sapply(links,grep,pattern="^actualité/")
)
unlist(sapply(links,grep,pattern="^actualité/"))
Scrap_link<- function(url){
url %>%
read_html()%>%
html_nodes("a")%>%
html_attr("href")%>%
unlist(,use.names = FALSE)
}
links <- lapply(paste0(url,c(1,2)),
Scrap_link)
View(links)
links
unlist(sapply(links,grep,pattern="^actualité/"))
Scrap_link<- function(url){
url %>%
read_html()%>%
html_nodes("a")%>%
html_attr("href")%>%
unique()
}
links <- lapply(paste0(url,c(1,2)),
Scrap_link)
unlist(sapply(links,grep,pattern="^actualité/"))
View(links)
links[[2]]
links[[1]]
sapply(links,grep,pattern="^actualité/"))
sapply(links,grep,pattern="^actualité/")
lapply(links,grep,pattern="^actualité/")
##Functions
scrap_lkeria <- function(x,l,i){
return(l[[x]][i[[x]]])
}
Thelinks <- sapply(c(1:2),scrap_lkeria,USE.NAMES = F)
Thelinks <- sapply(c(1:2),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
links <- lapply(paste0(url,c(1,2)),
Scrap_link)
indices <- lapply(links,grep,pattern="^actualité/")
##Functions
scrap_lkeria <- function(x,l,i){
return(l[[x]][i[[x]]])
}
Thelinks <- sapply(c(1:2),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
View(Thelinks)
unlist(Thelinks)
unlist(unlist(Thelinks))
do.call(c, unlist(Thelinks, recursive=FALSE))
do.call(c, unlist(unlist(Thelinks), recursive=FALSE))
do.call(c, unlist(Thelinks))
type(Thelinks)
class(Thelinks)
Scrap_link<- function(url){
url %>%
read_html()%>%
html_nodes("a")%>%
html_attr("href")%>%
unique()
}
links <- sapply(paste0(url,c(1,2)),
Scrap_link)
View(links)
indices <- Sapply(links,grep,pattern="^actualité/")
indices <- sapply(links,grep,pattern="^actualité/")
View(indices)
View(indices)
##Functions
scrap_lkeria <- function(x,l,i){
return(l[[x]][i[[x]]])
}
Thelinks <- sapply(c(1:2),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
View(Thelinks)
do.call(c, unlist(Thelinks, recursive=FALSE))
url <- "https://www.lkeria.com/actualité-P"
Scrap_link<- function(url){
url %>%
read_html()%>%
html_nodes("a")%>%
html_attr("href")%>%
unique()
}
links <- lapply(paste0(url,c(1,2)),
Scrap_link)
indices <- lapply(links,grep,pattern="^actualité/")
##Functions
scrap_lkeria <- function(x,l,i){
return(l[[x]][i[[x]]])
}
Thelinks <- sapply(c(1:2),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
View(Thelinks)
Thelinks <- lapply(c(1:2),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
View(Thelinks)
Thelinks[1]
Thelinks[[1]]
length(Thelinks)
Scrap_link<- function(url){
url %>%
read_html()%>%
html_nodes("a")%>%
html_attr("href")%>%
unique()
}
links <- lapply(paste0(url,c(1,72)),
Scrap_link)
indices <- lapply(links,grep,pattern="^actualité/")
##Functions
scrap_lkeria <- function(x,l,i){
return(l[[x]][i[[x]]])
}
Thelinks <- sapply(c(1:2),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
setwd("~/AlgerianRUser4thMeetUp")
length(Thelinks)
Thelinks <- sapply(c(1:72),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
links <- lapply(paste0(url,c(1,72)),
Scrap_link)
indices <- lapply(links,grep,pattern="^actualité/")
##Functions
scrap_lkeria <- function(x,l,i){
return(l[[x]][i[[x]]])
}
Thelinks <- sapply(c(1:72),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
length(links)
llinks
links
links <- lapply(paste0(url,c(1,72)),
Scrap_link)
length(links)
links <- lapply(paste0(url,c(1:72)),
Scrap_link)
indices <- lapply(links,grep,pattern="^actualité/")
##Functions
scrap_lkeria <- function(x,l,i){
return(l[[x]][i[[x]]])
}
Thelinks <- sapply(c(1:72),scrap_lkeria,USE.NAMES = F,l=links,i=indices)
length(Thelinks)
Thelinks <- sapply(c(1:72),scrap_lkeria,USE.NAMES = F,l=links,i=indices)%>%
unlist(use.names = F)
library(RCurl)
